---
title: Particle filtering tutorial
author: Jong-Hoon Kim
date: '2021-02-03'
slug: particle-filtering-tutorial
categories:
  - Parameter estimation
  - R
tags:
  - Particle Filter
  - Sequential Monte Carlo
  - Infectious disease epidemiology
subtitle: ''
summary: ''
authors: []
lastmod: '2021-02-03T12:07:57+09:00'
featured: no
image:
  caption: ''
  focal_point: ''
  preview_only: no
projects: []
---



<p>Particle filters operate on a set of randomly sampled values of a latent state or unknown parameter. The sampled values, generally referred to as ‘particles’, are propagated over time to track the posterior distribution of the state or parameter at each point in time. Each particle is assigned a weight in relation to its posterior probability.</p>
<p>Particle filtering is also known as:</p>
<ul>
<li>Sequential sampling-importance resampling (SIR)</li>
<li>Bootstrap filters</li>
<li>Condensation trackers</li>
<li>Interacting particle approximations</li>
<li>Survival of the fittest</li>
</ul>
<p>The foundations for the particle filtering is <em>sequential importance sampling and resampling</em>.
We will review item by item and start with the <em>importance sampling</em>.</p>
<p>Monte Carlo integration is basically to calculate (relative) area by to count the samples falling in the area of interest. It has the same underlying principle as calculating a sample average to approximate the population average. The law of large numbers ensures (almost surely) that we can approximate the population mean without a bias as long as we are provided with a large enough sample. Let’s suppose we will calculate the expected value of <span class="math inline">\(\sin(x)/x\)</span>
<img src="{{< blogdown/postref >}}index.en_files/figure-html/unnamed-chunk-1-1.png" width="672" /></p>
<div id="importance-sampling" class="section level3">
<h3>Importance sampling</h3>
<p>Importance sampling (IS) is a Monte Carlo integration technique and often used to integrate However, there are situations where obtaining samples that fall in the area of interest from the target distribution are not straightforward. In this case, we can use another distribution, called <em>instrumental distribution</em>, which can generate reasonable size of the samples.</p>
<p>We will estimate the proportion of successes that require over 3 seconds for a particular task, an example from the paper by Maarten Speekenbrink [1]. Response times (i.e., times that a person needs to complete a task) is often modeled as the ex-Gaussian distribution, which is defined as the sum of an exponential and Gaussian distributed variable.</p>
<p>Let’s first look at the ex-Gausssian distribution
<img src="{{< blogdown/postref >}}index.en_files/figure-html/unnamed-chunk-2-1.png" width="672" /></p>
<p>The task in hand is to determine the area under the curve where <span class="math inline">\(t &gt;= 3\)</span>. In this case, we know how to integrate the function so we don’t have to used MC integration. However, we will use MC integration for practice. True value of <span class="math inline">\(\int_3^{\infty} p(x) dx\)</span> is <span class="math inline">\(~ 0.00564\)</span> shown in the following code</p>
<pre class="r"><code>library(gamlss.dist) # for the exGAUS distribution
seed &lt;- 0
mu &lt;- .4; sigma &lt;- .1; tau &lt;- .5 # the true parameters
tm &lt;- 3 # the truncation point

(p &lt;- 1 - pexGAUS(tm, mu = mu, sigma = sigma, nu = tau)) # compute the true probability</code></pre>
<pre><code>## [1] 0.005628006</code></pre>
<p>We compute the integration by sampling (<span class="math inline">\(n = 2000\)</span>). Note that <span class="math inline">\(x\)</span> is sampled from</p>
<pre class="r"><code>set.seed(0) # set the seed to exactly reproduce results
N &lt;- 2000 # number of samples to use
nsim &lt;- 1 # number of times to repeat the computation, to show variability of the estimates
x &lt;- matrix(rnorm(N*nsim, mean = mu, sd = sigma) + rexp(N*nsim, rate = 1/tau), nrow = N, ncol = nsim)
# could have used x &lt;- matrix(rexGAUS(N*nsim,mu=mu,sigma=sigma,nu=tau),nrow=N,ncol=nsim) but this is terribly slow!
(epMC &lt;- apply(x, 2, function(z) sum(z &gt;= tm) / N)) # computed MC estimate for each simulation</code></pre>
<pre><code>## [1] 0.009</code></pre>
<p>We see that the estimate is quite far off (it may vary depending on the computer system). We can repeat the process above (setting nsim &gt; 1) to understand a variance of the estimate.</p>
<p>In the following we use</p>
<pre class="r"><code>library(msm) # for the truncated Normal distribution
library(gamlss.dist) # for the exGAUS distribution
## Importance Sampling estimate with a truncated exponential:
epIS1 &lt;- rep(0.0, nsim) # pre-assign a vector for the results
set.seed(seed) # set the seed to exactly reproduce results
x1 &lt;- matrix(rexp(N*nsim, rate = tau) + tm, nrow = N, ncol = nsim) # sample from a shifted exponential
w1 &lt;- dexGAUS(x1, mu = mu, sigma = sigma, nu = tau) / dexp(x1-tm, rate = tau) # compute importance weights
epIS1 &lt;- colSums(w1)/N # computed IS estimate for each simulation

## Importance Sampling estimate with a truncated normal:
epIS2 &lt;- rep(0.0, nsim) # pre-assign a vector for the results
set.seed(seed) # set the seed to exactly reproduce results
x2 &lt;- matrix(rtnorm(N*nsim, mean = tm, sd = sigma, lower = tm), nrow = N, ncol = nsim) # sample from a truncated Normal
w2 &lt;- matrix(dexGAUS(x2, mu = mu, sigma = sigma, nu = tau) / dtnorm(x2, mean = tm, sd = sigma, lower = tm), nrow = N, ncol = nsim) # compute importance weights
epIS2 &lt;- colSums(w2) / N # computed IS estimate for each simulation

## ----exgaussRep,echo=FALSE,warning=FALSE,fig.width=9,fig.height=3--------
require(ggplot2)
est &lt;- data.frame(ep = c(epMC,epIS1,epIS2),
                  method = factor(rep(1:3, each=nsim), labels = c(&quot;Basic Monte Carlo&quot;,&quot;Importance sampling (exponential)&quot;, &quot;Importance sampling (normal)&quot;)))
ggplot(est, aes(x=ep)) + 
  geom_histogram() + 
  facet_grid(. ~ method) + 
  xlim(c(.001,.01)) + 
  xlab(&quot;Estimate&quot;) + ylab(&quot;Count&quot;) + 
  geom_vline(xintercept = p, lty=2) + 
  theme(strip.text = element_text(size = rel(1.1)), axis.title=element_text(size = rel(1.5)))</code></pre>
<p><img src="{{< blogdown/postref >}}index.en_files/figure-html/unnamed-chunk-5-1.png" width="672" /></p>
</div>
<div id="sequential-importance-sampling-and-online-bayesian-inference" class="section level3">
<h3>Sequential importance sampling and online Bayesian inference</h3>
<p>We want to compute a sequence of posterior distributions <span class="math inline">\(p(\theta | y_1), p(\theta | y_{1:2}), ..., p(\theta | y_{1:t})\)</span> where <span class="math inline">\(y_{1:t} = (y_1, y_2, ..., y_t)\)</span> denotes a sequence of observations and <span class="math inline">\(\theta\)</span> a vector of parameters.</p>
<pre class="r"><code>### Here we use SIS to sequentially compute the posterior mean and SD of a Gaussian variable
set.seed(seed) # set the seed to exactly reproduce the results
# set the true parameters:
true_mu &lt;- 5
true_sigma &lt;- 5
nT &lt;- 100 # number of time points
# draw the observations:
y &lt;- rnorm(nT,mean=true_mu,sd=true_sigma) 

npart &lt;- 200 # number of particles
# draw particles from the prior distributions
sigmas &lt;- runif(npart,0,50)
mus &lt;- rnorm(npart,mean=0,sd=10)
# the weights are uniform as we sampled directly from the prior:
w &lt;- rep(1/npart, npart)
# Generate a matrix to store the weights:
W &lt;- matrix(0.0,ncol=nT,nrow=npart)
# iteratively update the weights
for(t in 1:nT) {
  w &lt;- w * dnorm(y[t], mean=mus, sd=sigmas)
  w &lt;- w/sum(w) #normalize
  W[,t] &lt;- w # store
}
# compute the posterior means and sd&#39;s
mu_post &lt;- colSums(mus*W)
sigma_post &lt;- colSums(sigmas*W)

## ----IteratedBayesianEstimationFigure,echo=FALSE,results=&#39;hide&#39;,warning=FALSE,fig.width=7,fig.height=3----
lowgrad = grey(.15)
highgrad = grey(.60)
### This plot is of the results of SIS for time-invariant parameters

gdat &lt;- data.frame(parameter=&quot;mu&quot;,
                   t=rep(1:nT,each=npart),
                   mu=rep(mus[order(mus)],nT),
                   w=as.numeric(W[order(mus),]),
                   ymin=rep(mus[order(mus)]-.5*c(1,diff(mus[order(mus)])),nT),
                   ymax=rep(mus[order(mus)]+.5*c(diff(mus[order(mus)]),1),nT),
                   xmin=rep(1:nT - .5,each=npart),
                   xmax=rep(1:nT + .5,each=npart))

gdat &lt;- rbind(gdat,
           data.frame(parameter=&quot;sigma&quot;,
                      t=rep(1:nT,each=npart),mu=rep(sigmas[order(sigmas)],nT),
                      w=as.numeric(W[order(sigmas),]),
                      ymin=rep(sigmas[order(sigmas)]-.5*c(1,diff(sigmas[order(sigmas)])),nT),
                      ymax=rep(sigmas[order(sigmas)]+.5*c(diff(sigmas[order(sigmas)]),1),nT),
                      xmin=rep(1:nT - .5,each=npart),
                      xmax=rep(1:nT + .5,each=npart)))
gdat$parameter &lt;- factor(gdat$parameter)

gdat2 &lt;- rbind(
  data.frame(parameter=&quot;mu&quot;,x=1:nT,y=mu_post),
  data.frame(parameter=&quot;sigma&quot;,x=1:nT,y=sigma_post))
gdat2$parameter &lt;- factor(gdat2$parameter,labels=c(expression(mu),expression(sigma)))

ggplot(gdat) + theme_bw() + 
  geom_rect(aes(fill=w,colour=w,ymin=ymin,ymax=ymax,xmin=xmin,xmax=xmax)) +
  geom_line(data=gdat2,aes(x=x,y=y),colour=&quot;white&quot;) +  
  geom_hline(yintercept = 5,colour=&quot;white&quot;,lty=2) + 
  facet_wrap(~parameter,labeller = &quot;label_parsed&quot;) + 
  coord_cartesian(ylim = c(.5, 10)) + xlab(&quot;t&quot;) + ylab(&quot;value&quot;) + 
  scale_fill_gradient(low=lowgrad,high=highgrad,trans=&quot;sqrt&quot;) + 
  scale_colour_gradient(low=lowgrad,high=highgrad,trans=&quot;sqrt&quot;) + 
  scale_x_continuous(limits=c(1,100),expand=c(0,0)) + 
  xlab(&quot;Time point (t)&quot;) + ylab(&quot;Value&quot;) + 
  theme(strip.text = element_text(size = rel(1.1)),axis.title=element_text(size = rel(1.5)))</code></pre>
<p><img src="{{< blogdown/postref >}}index.en_files/figure-html/unnamed-chunk-6-1.png" width="672" /></p>
</div>
<div id="sequential-importance-sampling-and-resampling" class="section level3">
<h3>Sequential importance sampling and resampling</h3>
<pre class="r"><code># Simulate the true latent process (theta) and observations (y)
set.seed(5:9)
mu0 &lt;- 10
sigma0 &lt;- sqrt(2)
sigmaxi &lt;- 1
sigmaeps &lt;- sqrt(10)
n &lt;- 50
theta &lt;- rnorm(1, mean=mu0, sd=sigma0) + c(0, cumsum(rnorm(n-1, mean=0, sd=sigmaxi)))
y &lt;- theta + rnorm(n, mean=0, sd=sigmaeps)

npart &lt;- 500 # number of particles to use

# Kalman Filter to analytically estimate the posterior mean
# mt contains posterior mean
# st contains posterior variance
mt &lt;- st &lt;- vector(&quot;double&quot;,length=n+1)
mt[1] &lt;- mu0
st[1] &lt;- sigma0^2
for(t in 1:n) {
    Kt &lt;- (st[t] + sigmaxi^2)/(st[t] + sigmaxi^2 + sigmaeps^2)
    st[t+1] &lt;- Kt*sigmaeps^2
    mt[t+1] &lt;- mt[t] + Kt*(y[t] - mt[t])
}

# The systematic resampling algorithm
resample_systematic &lt;- function(weights) {
  # input: weights is a vector of length N with (unnormalized) importance weights
  # output: a vector of length N with indices of the replicated particles
  N &lt;- length(weights)
  weights &lt;- weights/sum(weights)# normalize weights
  csum &lt;- cumsum(weights)
  u1 &lt;- runif(1,min=0,max=1/N) # draw a single uniform number
  u &lt;- c(0,seq(1/N,(N-1)/N,length=N-1)) + u1
  idx &lt;- vector(&quot;integer&quot;,length=length(weights))
  j &lt;- 1
  for(i in 1:N) {
    while (u[i] &gt; csum[j]) {
      j &lt;- j + 1
    }
    idx[i] &lt;- j
  }
  return(idx)
}

# Bootstrap filter
# create matrices to store the particle values and weights
Ta &lt;- Wa &lt;- matrix(NA,ncol=npart,nrow=n+1) 
set.seed(0) # set the seed to exactly replicate results
# draw the particles for the initial state from the prior distribution
Ta[1,] &lt;- rnorm(npart,mean=mu0,sd=sigma0)
# implicitly W &lt;- 1/npart
# loop over time
for(t in 1:n) {
    # sample particles according to the transition distribution
    Ta[t+1,] &lt;- rnorm(npart, mean=Ta[t,], sd = sigmaxi)
    # compute the weights
    Wa[t+1,] &lt;- dnorm(y[t], mean=Ta[t+1,], sd=sigmaeps) # could have multiplied by W, but not necessary with uniform weights
    Wa[t+1,] &lt;- Wa[t+1,]/sum(Wa[t+1,]) # normalize
    # draw indices of particles with systematic resampling
    idx &lt;- resample_systematic(Wa[t+1,])
    # implicitly W &lt;- 1/npart
    Ta[t+1,] &lt;- Ta[t+1,idx]
}

# The same filter without resampling (straightforward SIS)
Tb &lt;- Wb &lt;- matrix(NA,ncol=npart,nrow=n+1)
set.seed(0)
Tb[1,] &lt;- rnorm(npart,mean=mu0,sd=sigma0)
Wb[1,] &lt;- 1/npart
for(t in 1:n) {
    Tb[t+1,] &lt;- rnorm(npart,mean=Tb[t,],sd=sigmaxi)
    Wb[t+1,] &lt;- dnorm(y[t],mean=Tb[t+1,],sd=sigmaeps)*Wb[t,]
    Wb[t+1,] &lt;- Wb[t+1,]/sum(Wb[t+1,])
}

## ----GaussProcessFig,echo=FALSE,fig.width=6,fig.height=5-----------------
plot(1:n,y,ylab=&quot;Value&quot;,xlab=&quot;Time point (t)&quot;)
points(1:n,theta,pch=16)
lines(mt[-1],lty=1)
lines(rowSums(Ta*Wa)[-1],lty=2)
lines(rowSums(Tb*Wb)[-1],lty=3)
legend(c(0,max(y)),pch=c(1,16,NA,NA,NA),lty=c(NA,NA,1,2,3),
       legend=c(expression(y[t]),expression(phi[t]),&quot;posterior mean&quot;,&quot;Bootstrap filter (c=1)&quot;,&quot;SIS (c=0)&quot;),bty=&quot;n&quot;) </code></pre>
<p><img src="{{< blogdown/postref >}}index.en_files/figure-html/unnamed-chunk-7-1.png" width="672" /></p>
</div>
<div id="references" class="section level3">
<h3>References</h3>
<p>[1] A tutorial on particle filters Maarten Speekenbrink (2016) <em>Journal of Mathematical Psychology</em> 73:140–152</p>
</div>
